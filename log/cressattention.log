/media/ava/DATA2/Raj/rajkumar/lib/python3.8/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
Config:
 Config (path: configs/bevformer/bevformer_tiny.py): {'point_cloud_range': [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0], 'class_names': ['car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'], 'dataset_type': 'CustomNuScenesDataset', 'data_root': './data/nuscenes/', 'input_modality': {'use_lidar': False, 'use_camera': True, 'use_radar': False, 'use_map': False, 'use_external': True}, 'file_client_args': {'backend': 'disk'}, 'train_pipeline': [{'type': 'LoadMultiViewImageFromFiles', 'to_float32': True}, {'type': 'PhotoMetricDistortionMultiViewImage'}, {'type': 'LoadAnnotations3D', 'with_bbox_3d': True, 'with_label_3d': True, 'with_attr_label': False}, {'type': 'ObjectRangeFilter', 'point_cloud_range': [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]}, {'type': 'ObjectNameFilter', 'classes': ['car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone']}, {'type': 'NormalizeMultiviewImage', 'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'to_rgb': True}, {'type': 'RandomScaleImageMultiViewImage', 'scales': [0.5]}, {'type': 'PadMultiViewImage', 'size_divisor': 32}, {'type': 'DefaultFormatBundle3D', 'class_names': ['car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone']}, {'type': 'CustomCollect3D', 'keys': ['gt_bboxes_3d', 'gt_labels_3d', 'img']}], 'test_pipeline': [{'type': 'LoadMultiViewImageFromFiles', 'to_float32': True}, {'type': 'NormalizeMultiviewImage', 'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'to_rgb': True}, {'type': 'MultiScaleFlipAug3D', 'img_scale': (1600, 900), 'pts_scale_ratio': 1, 'flip': False, 'transforms': [{'type': 'RandomScaleImageMultiViewImage', 'scales': [0.5]}, {'type': 'PadMultiViewImage', 'size_divisor': 32}, {'type': 'DefaultFormatBundle3D', 'class_names': ['car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'], 'with_label': False}, {'type': 'CustomCollect3D', 'keys': ['img']}]}], 'eval_pipeline': [{'type': 'LoadPointsFromFile', 'coord_type': 'LIDAR', 'load_dim': 5, 'use_dim': 5, 'file_client_args': {'backend': 'disk'}}, {'type': 'LoadPointsFromMultiSweeps', 'sweeps_num': 10, 'file_client_args': {'backend': 'disk'}}, {'type': 'DefaultFormatBundle3D', 'class_names': ['car', 'truck', 'trailer', 'bus', 'construction_vehicle', 'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'], 'with_label': False}, {'type': 'Collect3D', 'keys': ['points']}], 'data': {'samples_per_gpu': 1, 'workers_per_gpu': 4, 'train': {'type': 'CustomNuScenesDataset', 'data_root': './data/nuscenes/', 'ann_file': './data/nuscenes/nuscenes_infos_temporal_train.pkl', 'pipeline': [{'type': 'LoadMultiViewImageFromFiles', 'to_float32': True}, {'type': 'PhotoMetricDistortionMultiViewImage'}, {'type': 'LoadAnnotations3D', 'with_bbox_3d': True, 'with_label_3d': True, 'with_attr_label': False}, {'type': 'ObjectRangeFilter', 'point_cloud_range': [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]}, {'type': 'ObjectNameFilter', 'classes': ['car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone']}, {'type': 'NormalizeMultiviewImage', 'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'to_rgb': True}, {'type': 'RandomScaleImageMultiViewImage', 'scales': [0.5]}, {'type': 'PadMultiViewImage', 'size_divisor': 32}, {'type': 'DefaultFormatBundle3D', 'class_names': ['car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone']}, {'type': 'CustomCollect3D', 'keys': ['gt_bboxes_3d', 'gt_labels_3d', 'img']}], 'classes': ['car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'], 'modality': {'use_lidar': False, 'use_camera': True, 'use_radar': False, 'use_map': False, 'use_external': True}, 'test_mode': False, 'box_type_3d': 'LiDAR', 'use_valid_flag': True, 'bev_size': (50, 50), 'queue_length': 3}, 'val': {'type': 'CustomNuScenesDataset', 'ann_file': './data/nuscenes/nuscenes_infos_temporal_val.pkl', 'pipeline': [{'type': 'LoadMultiViewImageFromFiles', 'to_float32': True}, {'type': 'NormalizeMultiviewImage', 'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'to_rgb': True}, {'type': 'MultiScaleFlipAug3D', 'img_scale': (1600, 900), 'pts_scale_ratio': 1, 'flip': False, 'transforms': [{'type': 'RandomScaleImageMultiViewImage', 'scales': [0.5]}, {'type': 'PadMultiViewImage', 'size_divisor': 32}, {'type': 'DefaultFormatBundle3D', 'class_names': ['car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'], 'with_label': False}, {'type': 'CustomCollect3D', 'keys': ['img']}]}], 'classes': ['car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'], 'modality': {'use_lidar': False, 'use_camera': True, 'use_radar': False, 'use_map': False, 'use_external': True}, 'test_mode': True, 'box_type_3d': 'LiDAR', 'data_root': './data/nuscenes/', 'bev_size': (50, 50), 'samples_per_gpu': 1}, 'test': {'type': 'CustomNuScenesDataset', 'data_root': './data/nuscenes/', 'ann_file': './data/nuscenes/nuscenes_infos_temporal_val.pkl', 'pipeline': [{'type': 'LoadMultiViewImageFromFiles', 'to_float32': True}, {'type': 'NormalizeMultiviewImage', 'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'to_rgb': True}, {'type': 'MultiScaleFlipAug3D', 'img_scale': (1600, 900), 'pts_scale_ratio': 1, 'flip': False, 'transforms': [{'type': 'RandomScaleImageMultiViewImage', 'scales': [0.5]}, {'type': 'PadMultiViewImage', 'size_divisor': 32}, {'type': 'DefaultFormatBundle3D', 'class_names': ['car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'], 'with_label': False}, {'type': 'CustomCollect3D', 'keys': ['img']}]}], 'classes': ['car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'], 'modality': {'use_lidar': False, 'use_camera': True, 'use_radar': False, 'use_map': False, 'use_external': True}, 'test_mode': True, 'box_type_3d': 'LiDAR', 'bev_size': (50, 50)}, 'shuffler_sampler': {'type': 'DistributedGroupSampler'}, 'nonshuffler_sampler': {'type': 'DistributedSampler'}}, 'evaluation': {'interval': 1, 'pipeline': [{'type': 'LoadMultiViewImageFromFiles', 'to_float32': True}, {'type': 'NormalizeMultiviewImage', 'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'to_rgb': True}, {'type': 'MultiScaleFlipAug3D', 'img_scale': (1600, 900), 'pts_scale_ratio': 1, 'flip': False, 'transforms': [{'type': 'RandomScaleImageMultiViewImage', 'scales': [0.5]}, {'type': 'PadMultiViewImage', 'size_divisor': 32}, {'type': 'DefaultFormatBundle3D', 'class_names': ['car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'], 'with_label': False}, {'type': 'CustomCollect3D', 'keys': ['img']}]}]}, 'checkpoint_config': {'interval': 1}, 'log_config': {'interval': 1, 'hooks': [{'type': 'TextLoggerHook'}, {'type': 'TensorboardLoggerHook'}]}, 'dist_params': {'backend': 'nccl'}, 'log_level': 'INFO', 'work_dir': None, 'load_from': None, 'resume_from': None, 'workflow': [('train', 1)], 'plugin': True, 'plugin_dir': 'projects/mmdet3d_plugin/', 'voxel_size': [0.2, 0.2, 8], 'img_norm_cfg': {'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'to_rgb': True}, '_dim_': 256, '_pos_dim_': 128, '_ffn_dim_': 512, '_num_levels_': 1, 'bev_h_': 50, 'bev_w_': 50, 'queue_length': 3, 'model': {'type': 'BEVFormer', 'export': False, 'use_grid_mask': True, 'video_test_mode': True, 'pretrained': {'img': 'torchvision://resnet50'}, 'img_backbone': {'type': 'ResNet', 'depth': 50, 'num_stages': 4, 'out_indices': (3,), 'frozen_stages': 1, 'norm_cfg': {'type': 'BN', 'requires_grad': False}, 'norm_eval': True, 'style': 'pytorch'}, 'img_neck': {'type': 'FPN', 'in_channels': [2048], 'out_channels': 256, 'start_level': 0, 'add_extra_convs': 'on_output', 'num_outs': 1, 'relu_before_extra_convs': True}, 'pts_bbox_head': {'type': 'BEVFormerHead', 'bev_h': 50, 'bev_w': 50, 'num_query': 900, 'num_classes': 10, 'in_channels': 256, 'sync_cls_avg_factor': True, 'with_box_refine': True, 'as_two_stage': False, 'transformer': {'type': 'PerceptionTransformer', 'rotate_prev_bev': True, 'use_shift': True, 'use_can_bus': True, 'embed_dims': 256, 'encoder': {'type': 'BEVFormerEncoder', 'num_layers': 3, 'pc_range': [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0], 'num_points_in_pillar': 4, 'return_intermediate': False, 'transformerlayers': {'type': 'BEVFormerLayer', 'attn_cfgs': [{'type': 'TemporalSelfAttention', 'embed_dims': 256, 'num_levels': 1}, {'type': 'SpatialCrossAttention', 'pc_range': [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0], 'deformable_attention': {'type': 'MSDeformableAttention3D', 'embed_dims': 256, 'num_points': 8, 'num_levels': 1}, 'embed_dims': 256}], 'feedforward_channels': 512, 'ffn_dropout': 0.1, 'operation_order': ('self_attn', 'norm', 'cross_attn', 'norm', 'ffn', 'norm')}}, 'decoder': {'type': 'DetectionTransformerDecoder', 'num_layers': 6, 'return_intermediate': True, 'transformerlayers': {'type': 'DetrTransformerDecoderLayer', 'attn_cfgs': [{'type': 'MultiheadAttention', 'embed_dims': 256, 'num_heads': 8, 'dropout': 0.1}, {'type': 'CustomMSDeformableAttention', 'embed_dims': 256, 'num_levels': 1}], 'feedforward_channels': 512, 'ffn_dropout': 0.1, 'operation_order': ('self_attn', 'norm', 'cross_attn', 'norm', 'ffn', 'norm')}}}, 'bbox_coder': {'type': 'NMSFreeCoder', 'post_center_range': [-61.2, -61.2, -10.0, 61.2, 61.2, 10.0], 'pc_range': [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0], 'max_num': 300, 'voxel_size': [0.2, 0.2, 8], 'num_classes': 10}, 'positional_encoding': {'type': 'LearnedPositionalEncoding', 'num_feats': 128, 'row_num_embed': 50, 'col_num_embed': 50}, 'loss_cls': {'type': 'FocalLoss', 'use_sigmoid': True, 'gamma': 2.0, 'alpha': 0.25, 'loss_weight': 2.0}, 'loss_bbox': {'type': 'L1Loss', 'loss_weight': 0.25}, 'loss_iou': {'type': 'GIoULoss', 'loss_weight': 0.0}}, 'train_cfg': {'pts': {'grid_size': [512, 512, 1], 'voxel_size': [0.2, 0.2, 8], 'point_cloud_range': [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0], 'out_size_factor': 4, 'assigner': {'type': 'HungarianAssigner3D', 'cls_cost': {'type': 'FocalLossCost', 'weight': 2.0}, 'reg_cost': {'type': 'BBox3DL1Cost', 'weight': 0.25}, 'iou_cost': {'type': 'IoUCost', 'weight': 0.0}, 'pc_range': [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]}}}}, 'optimizer': {'type': 'AdamW', 'lr': 0.0002, 'paramwise_cfg': {'custom_keys': {'img_backbone': {'lr_mult': 0.1}}}, 'weight_decay': 0.01}, 'optimizer_config': {'grad_clip': {'max_norm': 35, 'norm_type': 2}}, 'lr_config': {'policy': 'CosineAnnealing', 'warmup': 'linear', 'warmup_iters': 500, 'warmup_ratio': 0.3333333333333333, 'min_lr_ratio': 0.001}, 'total_epochs': 24, 'runner': {'type': 'EpochBasedRunner', 'max_epochs': 24}} 

projects.mmdet3d_plugin
/media/ava/DATA2/Raj/BEVFormer/src/projects/mmdet3d_plugin/bevformer/modules/custom_base_transformer_layer.py:94: UserWarning: The arguments `feedforward_channels` in BaseTransformerLayer has been deprecated, now you should set `feedforward_channels` and other FFN related arguments to a dict named `ffn_cfgs`. 
  warnings.warn(
/media/ava/DATA2/Raj/BEVFormer/src/projects/mmdet3d_plugin/bevformer/modules/custom_base_transformer_layer.py:94: UserWarning: The arguments `ffn_dropout` in BaseTransformerLayer has been deprecated, now you should set `ffn_drop` and other FFN related arguments to a dict named `ffn_cfgs`. 
  warnings.warn(
/media/ava/DATA2/Raj/BEVFormer/src/projects/mmdet3d_plugin/bevformer/modules/custom_base_transformer_layer.py:94: UserWarning: The arguments `ffn_num_fcs` in BaseTransformerLayer has been deprecated, now you should set `num_fcs` and other FFN related arguments to a dict named `ffn_cfgs`. 
  warnings.warn(
load checkpoint from local path: artifacts/bevformer_tiny_epoch_24.pth
The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.weight, pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.bias, pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.weight, pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.bias, pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight, pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias, pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.weight, pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.bias, pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.weight, pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.bias, pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.weight, pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.bias, pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.weight, pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.bias, pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.weight, pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.bias, pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.weight, pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.bias

missing keys in source state_dict: pts_bbox_head.transformer.encoder.layers.0.attentions.1.global_attention.to_q.0.weight, pts_bbox_head.transformer.encoder.layers.0.attentions.1.global_attention.to_q.0.bias, pts_bbox_head.transformer.encoder.layers.0.attentions.1.global_attention.to_q.1.weight, pts_bbox_head.transformer.encoder.layers.0.attentions.1.global_attention.to_k.0.weight, pts_bbox_head.transformer.encoder.layers.0.attentions.1.global_attention.to_k.0.bias, pts_bbox_head.transformer.encoder.layers.0.attentions.1.global_attention.to_k.1.weight, pts_bbox_head.transformer.encoder.layers.0.attentions.1.global_attention.to_v.0.weight, pts_bbox_head.transformer.encoder.layers.0.attentions.1.global_attention.to_v.0.bias, pts_bbox_head.transformer.encoder.layers.0.attentions.1.global_attention.to_v.1.weight, pts_bbox_head.transformer.encoder.layers.0.attentions.1.global_attention.proj.weight, pts_bbox_head.transformer.encoder.layers.0.attentions.1.global_attention.proj.bias, pts_bbox_head.transformer.encoder.layers.0.attentions.1.global_attention.prenorm.weight, pts_bbox_head.transformer.encoder.layers.0.attentions.1.global_attention.prenorm.bias, pts_bbox_head.transformer.encoder.layers.0.attentions.1.global_attention.mlp.0.weight, pts_bbox_head.transformer.encoder.layers.0.attentions.1.global_attention.mlp.0.bias, pts_bbox_head.transformer.encoder.layers.0.attentions.1.global_attention.mlp.2.weight, pts_bbox_head.transformer.encoder.layers.0.attentions.1.global_attention.mlp.2.bias, pts_bbox_head.transformer.encoder.layers.0.attentions.1.global_attention.postnorm.weight, pts_bbox_head.transformer.encoder.layers.0.attentions.1.global_attention.postnorm.bias, pts_bbox_head.transformer.encoder.layers.1.attentions.1.global_attention.to_q.0.weight, pts_bbox_head.transformer.encoder.layers.1.attentions.1.global_attention.to_q.0.bias, pts_bbox_head.transformer.encoder.layers.1.attentions.1.global_attention.to_q.1.weight, pts_bbox_head.transformer.encoder.layers.1.attentions.1.global_attention.to_k.0.weight, pts_bbox_head.transformer.encoder.layers.1.attentions.1.global_attention.to_k.0.bias, pts_bbox_head.transformer.encoder.layers.1.attentions.1.global_attention.to_k.1.weight, pts_bbox_head.transformer.encoder.layers.1.attentions.1.global_attention.to_v.0.weight, pts_bbox_head.transformer.encoder.layers.1.attentions.1.global_attention.to_v.0.bias, pts_bbox_head.transformer.encoder.layers.1.attentions.1.global_attention.to_v.1.weight, pts_bbox_head.transformer.encoder.layers.1.attentions.1.global_attention.proj.weight, pts_bbox_head.transformer.encoder.layers.1.attentions.1.global_attention.proj.bias, pts_bbox_head.transformer.encoder.layers.1.attentions.1.global_attention.prenorm.weight, pts_bbox_head.transformer.encoder.layers.1.attentions.1.global_attention.prenorm.bias, pts_bbox_head.transformer.encoder.layers.1.attentions.1.global_attention.mlp.0.weight, pts_bbox_head.transformer.encoder.layers.1.attentions.1.global_attention.mlp.0.bias, pts_bbox_head.transformer.encoder.layers.1.attentions.1.global_attention.mlp.2.weight, pts_bbox_head.transformer.encoder.layers.1.attentions.1.global_attention.mlp.2.bias, pts_bbox_head.transformer.encoder.layers.1.attentions.1.global_attention.postnorm.weight, pts_bbox_head.transformer.encoder.layers.1.attentions.1.global_attention.postnorm.bias, pts_bbox_head.transformer.encoder.layers.2.attentions.1.global_attention.to_q.0.weight, pts_bbox_head.transformer.encoder.layers.2.attentions.1.global_attention.to_q.0.bias, pts_bbox_head.transformer.encoder.layers.2.attentions.1.global_attention.to_q.1.weight, pts_bbox_head.transformer.encoder.layers.2.attentions.1.global_attention.to_k.0.weight, pts_bbox_head.transformer.encoder.layers.2.attentions.1.global_attention.to_k.0.bias, pts_bbox_head.transformer.encoder.layers.2.attentions.1.global_attention.to_k.1.weight, pts_bbox_head.transformer.encoder.layers.2.attentions.1.global_attention.to_v.0.weight, pts_bbox_head.transformer.encoder.layers.2.attentions.1.global_attention.to_v.0.bias, pts_bbox_head.transformer.encoder.layers.2.attentions.1.global_attention.to_v.1.weight, pts_bbox_head.transformer.encoder.layers.2.attentions.1.global_attention.proj.weight, pts_bbox_head.transformer.encoder.layers.2.attentions.1.global_attention.proj.bias, pts_bbox_head.transformer.encoder.layers.2.attentions.1.global_attention.prenorm.weight, pts_bbox_head.transformer.encoder.layers.2.attentions.1.global_attention.prenorm.bias, pts_bbox_head.transformer.encoder.layers.2.attentions.1.global_attention.mlp.0.weight, pts_bbox_head.transformer.encoder.layers.2.attentions.1.global_attention.mlp.0.bias, pts_bbox_head.transformer.encoder.layers.2.attentions.1.global_attention.mlp.2.weight, pts_bbox_head.transformer.encoder.layers.2.attentions.1.global_attention.mlp.2.bias, pts_bbox_head.transformer.encoder.layers.2.attentions.1.global_attention.postnorm.weight, pts_bbox_head.transformer.encoder.layers.2.attentions.1.global_attention.postnorm.bias

[                                                  ] 0/81, elapsed: 0s, ETA:/media/ava/DATA2/Raj/BEVFormer/src/projects/mmdet3d_plugin/bevformer/modules/transformer.py:140: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)
  shift = bev_queries.new_tensor(
/media/ava/DATA2/Raj/rajkumar/lib/python3.8/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
torch.Size([1, 6, 256, 50, 50]) torch.Size([1, 6, 15, 25, 256]) torch.Size([1, 6, 15, 25, 256])
Traceback (most recent call last):
  File "src/tools/test.py", line 306, in <module>
    main()
  File "src/tools/test.py", line 275, in main
    outputs = custom_multi_gpu_test(model, data_loader, args.tmpdir,
  File "/media/ava/DATA2/Raj/BEVFormer/src/projects/mmdet3d_plugin/bevformer/apis/test.py", line 72, in custom_multi_gpu_test
    result = model(return_loss=False, rescale=True, **data)
  File "/media/ava/DATA2/Raj/rajkumar/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/media/ava/DATA2/Raj/rajkumar/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1008, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/media/ava/DATA2/Raj/rajkumar/lib/python3.8/site-packages/mmcv/parallel/distributed.py", line 165, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore
  File "/media/ava/DATA2/Raj/rajkumar/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/media/ava/DATA2/Raj/BEVFormer/src/projects/mmdet3d_plugin/bevformer/detectors/bevformer.py", line 167, in forward
    return self.forward_test(**kwargs)
  File "/media/ava/DATA2/Raj/BEVFormer/src/projects/mmdet3d_plugin/bevformer/detectors/bevformer.py", line 293, in forward_test
    new_prev_bev, bbox_results = self.simple_test(
  File "/media/ava/DATA2/Raj/BEVFormer/src/projects/mmdet3d_plugin/bevformer/detectors/bevformer.py", line 328, in simple_test
    new_prev_bev, bbox_pts = self.simple_test_pts(
  File "/media/ava/DATA2/Raj/BEVFormer/src/projects/mmdet3d_plugin/bevformer/detectors/bevformer.py", line 303, in simple_test_pts
    outs = self.pts_bbox_head(x, img_metas, prev_bev=prev_bev)
  File "/media/ava/DATA2/Raj/rajkumar/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/media/ava/DATA2/Raj/rajkumar/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/media/ava/DATA2/Raj/BEVFormer/src/projects/mmdet3d_plugin/bevformer/dense_heads/bevformer_head.py", line 176, in forward
    outputs = self.transformer(
  File "/media/ava/DATA2/Raj/rajkumar/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/media/ava/DATA2/Raj/rajkumar/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/media/ava/DATA2/Raj/BEVFormer/src/projects/mmdet3d_plugin/bevformer/modules/transformer.py", line 435, in forward
    bev_embed = self.get_bev_features(
  File "/media/ava/DATA2/Raj/rajkumar/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/media/ava/DATA2/Raj/BEVFormer/src/projects/mmdet3d_plugin/bevformer/modules/transformer.py", line 219, in get_bev_features
    bev_embed = self.encoder(
  File "/media/ava/DATA2/Raj/rajkumar/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/media/ava/DATA2/Raj/rajkumar/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/media/ava/DATA2/Raj/BEVFormer/src/projects/mmdet3d_plugin/bevformer/modules/encoder.py", line 324, in forward
    output = layer(
  File "/media/ava/DATA2/Raj/rajkumar/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/media/ava/DATA2/Raj/BEVFormer/src/projects/mmdet3d_plugin/bevformer/modules/encoder.py", line 492, in forward
    query = self.attentions[attn_index](
  File "/media/ava/DATA2/Raj/rajkumar/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/media/ava/DATA2/Raj/rajkumar/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py", line 208, in new_func
    return old_func(*args, **kwargs)
  File "/media/ava/DATA2/Raj/BEVFormer/src/projects/mmdet3d_plugin/bevformer/modules/spatial_cross_attention.py", line 282, in forward
    queries =self.global_attention(queries_rebatch,key,value)
  File "/media/ava/DATA2/Raj/rajkumar/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/media/ava/DATA2/Raj/BEVFormer/src/projects/mmdet3d_plugin/bevformer/modules/spatial_cross_attention.py", line 362, in forward
    torch.einsum('b n Q c d, b n Q K d -> b n Q c K', q, k)
  File "/media/ava/DATA2/Raj/rajkumar/lib/python3.8/site-packages/torch/functional.py", line 360, in einsum
    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
RuntimeError: einsum(): operands do not broadcast with remapped shapes [original->remapped]: [4, 6, 2500, 1, 12]->[4, 6, 2500, 1, 1, 12] [4, 6, 15, 25, 12]->[4, 6, 15, 1, 25, 12]
/media/ava/DATA2/Raj/rajkumar/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3516847) of binary: /media/ava/DATA2/Raj/rajkumar/bin/python
Traceback (most recent call last):
  File "/media/ava/DATA2/Raj/rajkumar/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/media/ava/DATA2/Raj/rajkumar/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/media/ava/DATA2/Raj/rajkumar/lib/python3.8/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/media/ava/DATA2/Raj/rajkumar/lib/python3.8/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/media/ava/DATA2/Raj/rajkumar/lib/python3.8/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/media/ava/DATA2/Raj/rajkumar/lib/python3.8/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/media/ava/DATA2/Raj/rajkumar/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/media/ava/DATA2/Raj/rajkumar/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
src/tools/test.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-04_10:06:56
  host      : benz
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3516847)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
